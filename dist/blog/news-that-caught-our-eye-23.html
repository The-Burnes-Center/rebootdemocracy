<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><link rel="shortcut icon" type="image/png" href="/innovateus_favicon.001.png"><link rel="stylesheet" href="https://kit.fontawesome.com/59ddbfe387.css" crossorigin="anonymous"><link href="https://cdn.jsdelivr.net/npm/@mdi/font@5.x/css/materialdesignicons.min.css" rel="stylesheet"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0"><meta name="viewport" content="width=device-width,initial-scale=1"><script type="module" async="" crossorigin="" src="./assets/app-sZ1ocDrj.js"></script><link rel="stylesheet" crossorigin="" href="./assets/app-CYsdt1SR.css"><link rel="modulepreload" crossorigin="" href="./assets/_slug_-Cc2N-6pe.js"></head><body><div id="app" class="grid justify-items-stretch" data-server-rendered="true"><div class="app-container" data-v-390d8bb0=""><div class="main-content" data-v-390d8bb0=""><div data-v-390d8bb0=""><div><h1>News That Caught Our Eye #23: August 7, 2024</h1><div><p dir="ltr"><a href="https://ceur-ws.org/Vol-3737/paper7.pdf">Using Artificial Intelligence in Parliament - Initial Results from the Canadian House of Commons</a> – CEUR Workshop Proceedings, by Jörn von Lucke, Fotios Fitsilis, and Stéphane Gagnon, August 3, 2024</p><ul><li dir="ltr" aria-level="1"><p dir="ltr" role="presentation">“Parliaments are already exploring the integration of artificial intelligence (AI) technology for specific tasks. Reflecting on possible tools, application areas, usage scenarios, and requirements, it is reasonable to anticipate that AI-driven changes will manifest in parliamentary operations. Though Canada has been championing AI, additional research is necessary for its seamless integration and use in the parliamentary workspace. This research paper contributes to the bridging of this gap by presenting empirical evidence for the future use of AI-based tools and services, along with addressing open questions for their implementation within the Canadian Parliament. The data were collected during a brainstorming exercise in July 2020 and a virtual workshop in September 2023. An examination was conducted to investigate the relevance and priority of 210 applications and topics related to parliamentary AI.”</p></li></ul><p><strong>&nbsp;</strong></p><p dir="ltr"><a href="https://demoshelsinki.fi/wp-content/uploads/2024/07/Demos-Helsinki-Generative-shared-intelligence-and-the-future-shape-of-government-Geoff-Mulgan.pdf">Generative Shared Intelligence (GSI): A direction for governments in the uncertain environment of the late 2020s</a> – Demos Helsinki, by Geoff Mulgan, July 2024</p><ul><li dir="ltr" aria-level="1"><p dir="ltr" role="presentation">“....The focus here is on the ‘how’ rather than the ‘what’. But everything I argue becomes more important if governments have bigger tasks on their plate and tougher constraints. The core argument is that governments need to mobilise what I call ‘generative shared intelligence’, that increasingly complements the traditional focus of governments on law and finance. This provides a broader framework for understanding and shaping the role of data and AI, citizen engagement and evidence, and much more. It’s relevant both to the internal plumbing of governments – how they achieve results – but also to the renewal of democracy since it focuses attention on the gold in peoples’ heads, the insights of citizens, workers and families, as well as hardware and data.”</p></li></ul><p><strong>&nbsp;</strong></p><p dir="ltr"><a href="https://demnext.substack.com/p/how-a-permanent-citizens-assembly">How a permanent Citizens' Assembly in Paris passed a bill into law</a> – Democracy Next, July 25, 2024</p><ul><li dir="ltr" aria-level="1"><p dir="ltr" role="presentation">“On July 10, for the first time in history, the Paris City Council took up major legislation written by a permanent Citizens’ Assembly, composed of 100 regular people, and passed it directly into law.&nbsp; To understand how this happened, we spoke (in French) to Elian Belon, the General Secretary of the Paris Citizens’ Assembly. ‘For this to be a success, you need a strong political will,’ Belon told us. ‘The executive was really involved. The mayor was very supportive and has followed really closely. The politicians were very impressed by the capacity of the citizens.’&nbsp; In an interview, Belon described the process from start to finish. The Paris Citizens’ Assembly was established in 2021, designed by a group including DemocracyNext Founder/CEO Claudia Chwalisz and Federation for Innovation in Democracy (FIDE).”</p></li></ul><p><strong>&nbsp;</strong></p><p dir="ltr"><a href="https://dl.acm.org/doi/10.1145/3657604.3664693">Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs Ability To Mark Short Answer Questions in K-12 Education</a> – L@S '24: Proceedings of the Eleventh ACM Conference on Learning @ Scale, by Owen Henkel et al., July 15, 2024</p><ul><li dir="ltr" aria-level="1"><p dir="ltr" role="presentation">“This paper presents reports on a series of experiments with a novel dataset evaluating how well Large Language Models (LLMs) can mark (i.e. grade) open text responses to short answer questions, Specifically, we explore how well different combinations of GPT version and prompt engineering strategies performed at marking real student answers to short answer across different domain areas (Science and History) and grade-levels (spanning ages 5-16) using a new, never-used-before dataset from Carousel, a quizzing platform. We found that GPT-4, with basic few-shot prompting performed well (Kappa, 0.70) and, importantly, very close to human-level performance (0.75). This research builds on prior findings that GPT-4 could reliably score short answer reading comprehension questions at a performance-level very close to that of expert human raters. The proximity to human-level performance, across a variety of subjects and grade levels suggests that LLMs could be a valuable tool for supporting low-stakes formative assessment tasks in K-12 education and has important implications for real-world education delivery.”</p></li></ul><p><strong>&nbsp;</strong></p><p dir="ltr"><a href="https://www.governing.com/policy/can-ai-help-kentuckys-public-defenders#:~:text=Throughout%20this%20imbalance%2C%20a%20new,field%20and%20ensure%20fair%20defense.">Can AI Help Kentucky's Public Defendants?</a> – Governing, by Taylor Six, August 6, 2024</p><ul><li dir="ltr" aria-level="1"><p dir="ltr" role="presentation">“The Kentucky Department of Public Advocacy struggles with an overwhelming number of cases with legally challenged clients. This puts pressure on its resources and threaten[s] the surety of a fair trial to the people it represents. Meanwhile, Kentucky prosecutors benefit from higher funding and expanded support, which means public defense attorneys have to equip themselves with tools that can break the status quo. Throughout this imbalance, a new AI-powered tool called JusticeText — exclusive to public defenders — has emerged as a potential aid, offering public defenders a crucial boost in analyzing case evidence efficiently, in efforts to level the playing field and ensure fair defense.”</p></li></ul><p><strong>&nbsp;</strong></p><p dir="ltr"><a href="https://www.sacbee.com/news/politics-government/the-state-worker/article290596589.html">California hopes to bolster AI preparedness with employee training for state workers</a> – The Sacramento Bee, by William Melhado, July 31, 2024</p><ul><li dir="ltr" aria-level="1"><p dir="ltr" role="presentation">“California hopes to build state workers’ artificial intelligence credentials through an optional training that teaches staff how to use the rapidly evolving technology. The California Department of Human Resources unveiled a series of professional development courses for public employees this month following Gov. Gavin Newsom’s September executive order that instructed state agencies to brainstorm and develop a plan for how to ‘ethically and responsibly’ deploy AI technology in government operations. When California began exploring using AI, the state spoke with academics, labor groups, industry partners and community organizations to determine common concerns, Secretary of Government Operations Amy Tong said.”</p></li></ul><p><strong>&nbsp;</strong></p><p dir="ltr"><a href="https://www.linkedin.com/pulse/escaping-ais-competency-trap-what-training-ourselves-just-weiss-no9ge/">Escaping AI's Competency Trap: What Training Ourselves (and Not Just Our Robots) Could Accomplish</a> – Chief of Stuff, by Mitchell Weiss, August 5, 2024</p><ul><li dir="ltr" aria-level="1"><p dir="ltr" role="presentation">“....Effective generative AI training isn't designed to make moral converts, but it should help people begin to explore the tools. It’s been almost two year[s] since ChatGPT burst onto the scene. People marvel at its race to more than 200 million users. (Not to mention Gemini’s, Claude’s, Copilot’s, and all the rest.) But what’s perhaps just as striking if you peek inside many organizations today as the number of people who are using AI is the number of people who aren’t. There are many reasons for this. And some remedies. One reason for laggard generative AI adoption is AI's Competency Trap. One remedy is effective training.”&nbsp;</p></li></ul><p><strong>&nbsp;</strong></p><p dir="ltr"><a href="https://www.govtech.com/biz/data/nycs-data-driven-future-46-algorithms-and-counting">NYC’s Data-Driven Future: 46 Algorithms and Counting</a> – Government Technology, by Nikki Davidson, July 31, 2024</p><ul><li dir="ltr" aria-level="1"><p dir="ltr" role="presentation">“New York City runs on algorithms, at least 46 different algorithmic tools to be exact. Behind the rapid expansion of these tools is an example of how America’s biggest city is aiming to be transparent about which departments use these tools and what kind of data is being processed and by whom. In a decade, the number of reported algorithmic tools used in New York City has surged from eight to 46, highlighting the city’s growing reliance on digital solutions. These tools touch the lives of every New Yorker, often without their awareness, impacting everything from public safety and transportation to social services and education.”</p></li></ul><p><strong>&nbsp;</strong></p><p dir="ltr"><a href="https://apnews.com/article/election-2024-misinformation-x-twitter-elon-musk-230cc0e99b3f490585bdff66a29e079a">Secretaries of state urge Elon Musk to fix AI chatbot spreading election misinformation on X</a> – AP News, by Christine Fernando, August 5, 2024</p><ul><li dir="ltr" aria-level="1"><p dir="ltr" role="presentation">“Five secretaries of state are urging Elon Musk to fix an AI chatbot on the social media platform X, saying in a letter sent Monday that it has spread election misinformation. The top election officials from Michigan, Minnesota, New Mexico, Pennsylvania and Washington told Musk that X’s AI chatbot, Grok, produced false information about state ballot deadlines shortly after President Joe Biden dropped out of the 2024 presidential race. While Grok is available only to subscribers to the premium versions of X, the misinformation was shared across multiple social media platforms and reached millions of people, according to the letter.”</p></li></ul><p><strong>&nbsp;</strong></p><p dir="ltr"><a href="https://www.nytimes.com/interactive/2024/07/18/technology/spain-domestic-violence-viogen-algorithm.html">An Algorithm Told Police She Was Safe. Then Her Husband Killed Her.</a> – New York Times, by Adam Satariano and Roser Toll Pifarré, July 18, 2024.</p><ul><li dir="ltr" aria-level="1"><p dir="ltr" role="presentation">“....Spain has become dependent on an algorithm to combat gender violence, with the software so woven into law enforcement that it is hard to know where its recommendations end and human decision-making begins. At its best, the system has helped police protect vulnerable women and, overall, has reduced the number of repeat attacks in domestic violence cases. But the reliance on VioGén has also resulted in victims, whose risk levels are miscalculated, getting attacked again — sometimes leading to fatal consequences. Spain now has 92,000 active cases of gender violence victims who were evaluated by VioGén, with most of them — 83 percent — classified as facing little risk of being hurt by their abuser again. Yet roughly 8 percent of women who the algorithm found to be at negligible risk and 14 percent at low risk have reported being harmed again, according to Spain’s Interior Ministry, which oversees the system.”</p></li></ul><p><strong>&nbsp;</strong></p><p dir="ltr"><a href="https://www.bbc.com/news/articles/c6233x9k4dlo">AI in healthcare: what are the risks for the NHS?</a> – BBC, by Katharine da Costa, August 7, 2024</p><ul><li dir="ltr" aria-level="1"><p dir="ltr" role="presentation">“At a time when there are more than seven million patients on the NHS waiting list in England and around 100,000 staff vacancies, artificial intelligence could revolutionise the health service by improving patient care and freeing up staff time. Its uses are varied - from spotting risk factors in a bid to help prevent chronic conditions such as heart attacks, strokes and diabetes - to assisting clinicians by analysing scans and x-rays to speed up diagnosis. The technology is also maximising productivity by carrying out routine administrative tasks from automated voice assistants to scheduling appointments and capturing doctors' consultation notes….AI opens up a world of possibilities, but it brings risks and challenges too, like maintaining accuracy.”</p></li></ul><p>&nbsp;</p></div></div></div></div></div></div></body></html>