<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><link rel="shortcut icon" type="image/png" href="/innovateus_favicon.001.png"><link rel="stylesheet" href="https://kit.fontawesome.com/59ddbfe387.css" crossorigin="anonymous"><link href="https://cdn.jsdelivr.net/npm/@mdi/font@5.x/css/materialdesignicons.min.css" rel="stylesheet"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0"><meta name="viewport" content="width=device-width,initial-scale=1"><script type="module" async="" crossorigin="" src="./assets/app-sZ1ocDrj.js"></script><link rel="stylesheet" crossorigin="" href="./assets/app-CYsdt1SR.css"><link rel="modulepreload" crossorigin="" href="./assets/_slug_-Cc2N-6pe.js"></head><body><div id="app" class="grid justify-items-stretch" data-server-rendered="true"><div class="app-container" data-v-390d8bb0=""><div class="main-content" data-v-390d8bb0=""><div data-v-390d8bb0=""><div><h1>News That Caught Our Eye #15: May 8th, 2024</h1><div><p>If you have an item that we should include in this news download, or a source we should review for future items, please email me at kemp.j@northeastern.edu.</p><h3 dir="ltr">OpenAI Releases ‘Deepfake’ Detector to Disinformation Researchers</h3><p dir="ltr"><a href="https://nytimes.com/2024/05/07/technology/openai-deepfake-detector.html">The New York Times</a> (May 7, 2024)</p><p dir="ltr">Today, OpenAI announced it will share its new tool to detect AI deepfakes – including its own– with a small group of disinformation researchers, in hopes of real-world testing and pinpointing ways it could be improved. The developer said its new detector could correctly identify 98.8 percent of images created by its latest image-generating tool, DALL-E 3. This comes alongside a series of other efforts by OpenAI to disrupt deepfake-led misinformation, including recently joining the steering committee for the Coalition for Content Provenance and Authenticity.</p><p><strong>&nbsp;</strong></p><h3 dir="ltr">Microsoft and OpenAI launch $2M fund to counter election deepfakes</h3><p dir="ltr"><a href="https://techcrunch.com/2024/05/07/microsoft-and-openai-launch-2m-fund-to-counter-election-deepfakes/">TechCrunch</a> (May 7, 2024)</p><p dir="ltr">To further help stave off the risk of AI-generated deepfakes ahead of the upcoming election, Microsoft and OpenAI have jointly announced the Societal Resilience Fund – which involves issuing grants to a handful of organizations to further AI education and literacy among voters and vulnerable communities. One provided example in the article: grant recipient Older Adults Technology Services has said it will use its grant for training programs on foundational AI understanding for those aged 50 and over.</p><p><strong>&nbsp;</strong></p><h3 dir="ltr">Microsoft deploys air-gapped AI for classified defense, intelligence customers</h3><p dir="ltr"><a href="https://www.nextgov.com/artificial-intelligence/2024/05/microsoft-deploys-air-gapped-ai-classified-cloud/396354/">NextGov</a> (May 7, 2024)</p><p dir="ltr">Microsoft has deployed a generative AI model entirely divorced from the internet for the first time, safe for both classified U.S. government workloads and intelligence agencies, which can now safely harness the technology to analyze top-secret information. End users on DOD's classified network will be able to use the generative AI toolkit, but they will not have the ability to train the model itself on new information and data because the model is air-gapped – meaning the secure computer network is physically isolated from other unsecured networks.</p><p>&nbsp;</p><h3 dir="ltr">The House’s AI Task Force leader says regulation can’t be rushed</h3><p dir="ltr"><a href="https://www.fastcompany.com/91117132/jay-obernolte-interview-house-ai-task-force">Fast Company</a> (May 3, 2024)</p><p dir="ltr">In an interview with California Congressman Jay Obernolte (R), <em>Fast Company</em> asked the co-chair of the bipartisan Task Force on Artificial Intelligence about the task force’s mission and his perspective on “going slow” with crafting regulatory federal legislation. Obernolte specifically references a variety of concerns around engaging all stakeholders, allowing industry capture, the complexity of IP law, and whether to follow the lead of entities like the European Union or empower existing regulators. Read the interview for his full comments.</p><p dir="ltr">&nbsp;</p><h3 dir="ltr">National Cybersecurity, AI, and Congress: An Interview with Will Hurd</h3><p dir="ltr"><a href="https://harvardpolitics.com/national-cybersecurity-ai-and-congress-an-interview-with-will-hurd/">Harvard Political Review</a> (May 3, 2024)</p><p dir="ltr">In an interview with former CIA officer and Texas Congressman Will Hurd, <em>Harvard Political Review</em> asked the politician a series of questions about security and privacy regarding new technological threats. From a regulatory perspective, Hurd’s first concern is that AI has to follow existing law on civil rights and civil liberties. He also believes privacy should be protected at all costs, including strengthening encryption in federal IT systems and cooperation between the industry, the public, and the government to protect against foreign government threats. For the CIA specifically, Hurd also sees a strong AI use case for considering large bodies of information – if the agency can maintain information security.</p><p><strong>&nbsp;</strong></p><h3 dir="ltr">What’s Next for AI in States? An AI Sandbox</h3><p dir="ltr"><a href="https://www.govtech.com/artificial-intelligence/whats-next-for-ai-in-states-an-ai-sandbox">GovTech</a> (May 3, 2024)</p><p dir="ltr">At the NASCIO Midyear Conference this week, Massachusetts CIO Jason Snyder used The Burnes Center’s <a href="https://burnes.northeastern.edu/ai-for-impact-coop/">AI for Impact program</a> as an example of a “sandbox” – the kinds of working groups states are turning to for concrete use cases that will help government run and provide public services more efficiently. Georgie is also working on a similar lab, which will allow agencies to experiment with AI tools and find uses and failures “fast.”</p><p><strong>&nbsp;</strong></p><h3 dir="ltr">California and Other States Threaten to Derail the AI Revolution</h3><p dir="ltr"><a href="https://www.rstreet.org/commentary/california-and-other-states-threaten-to-derail-the-ai-revolution/">R Street</a> (May 2, 2024)</p><p dir="ltr">California is leading the push for aggressive state legislation on AI, as state lawmakers nationally consider creating the equivalent of 50 different computational control commissions as part of their individual regulatory agendas. This move is driven by both fear and Congressional hesitancy to overregulate too quickly, and could lead to a confusing patchwork of state AI regulations that undermine innovation, investment, and competition.</p><p dir="ltr">&nbsp;</p><h3 dir="ltr">New NSF grant targets large language models and generative AI, exploring how they work and implications for societal impacts</h3><p dir="ltr"><a href="https://new.nsf.gov/news/new-nsf-grant-targets-large-language-models">U.S. National Science Foundation</a> (May 2, 2024)</p><p dir="ltr">Northeastern University, home of the Burnes Center for Social Change, has received a $9 million grant “to investigate how large language models (LLMs) and generative AI operate, focusing on the computing process called deep inference and AI’s long-term societal impacts.” To provide researchers and academics with a transparent platform to observe the internal computations of large-language models, Northeastern’s research will aim to establish a <a href="https://ndif.us/">National Deep Inference Fabric</a> and address the gap between industry and academia.</p><p dir="ltr">&nbsp;</p><h3 dir="ltr">The EU AI Act: Final Text Published</h3><p dir="ltr"><a href="https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138-FNL-COR01_EN.pdf?utm_source=substack&amp;utm_medium=email">European Parliament</a> (April 30, 2024)</p><p dir="ltr">“On 13 March 2024, the European Parliament adopted the AI Act at a first reading. Now, the corrigendum of the text has been released. This document corrected the language in the Act.” -Risto Uuk, Future of Life Institute</p><p dir="ltr">&nbsp;</p><h3 dir="ltr">The limits of state AI legislation</h3><p dir="ltr"><a href="https://www.politico.com/newsletters/digital-future-daily/2024/04/30/the-limits-of-state-ai-legislation-00155265?utm_campaign=Newsletters&amp;utm_medium=email&amp;utm_source=sendgrid">Politico</a> (April 30, 2024)</p><p dir="ltr">As Congress remains largely stalled on passing federal regulations for AI, every state <a href="https://www.axios.com/2024/02/14/ai-bills-state-legislatures-deepfakes-bias-discrimination">except Alabama and Wyoming</a> is currently considering “some kind” of AI legislation. However, two consumer advocates warn <em>Politico</em> that most state laws “are overlooking crucial loopholes that could shield companies from liability when it comes to harm caused by AI decisions — or from simply being forced to disclose when it’s used in the first place.” Much of this is due to haziness around what constitutes a “trade secret,” as well as jargon around which systems are designed to be “controlling” or a “substantive” factor in decision-making.</p><p dir="ltr">&nbsp;</p><h3 dir="ltr">Generative AI is already helping fact-checkers. But it’s proving less useful in small languages and outside the West</h3><p dir="ltr"><a href="https://reutersinstitute.politics.ox.ac.uk/news/generative-ai-already-helping-fact-checkers-its-proving-less-useful-small-languages-and">Reuters Institute</a> (April 29, 2024)</p><p dir="ltr">Journalism experts from Norway, Georgia and Ghana are applying artificial intelligence tools in their fact-checking efforts – including the “limitations of this new technology when applied to diverse geographical contexts, for example in non-Western countries or in countries with underrepresented languages in training models.” In Ghana, content moderators are also struggling to keep up with AI-generated content and troll farms that are driving election narratives. Be sure to read the whole article for interesting case studies and tool examples for AI-powered fact-checking.</p><p dir="ltr">&nbsp;</p><h3 dir="ltr">Here's How Generative AI Depicts Queer People</h3><p dir="ltr"><a href="https://www.wired.com/story/artificial-intelligence-lgbtq-representation-openai-sora/">WIRED</a> (April 2, 2024)</p><p dir="ltr">Revisit this WIRED investigation from early April, which examines how many current AI tools and image-generators portray queer people – think: white people with purple hair and lots of piercings. Harmful biases are difficult to train out of models, but one strategy is to ensure LLMs “focus on well-labeled data that includes additional representations of LGBTQ people from around the world.” Scroll to the bottom for some interesting outputs from Sora, OpenAI’s new video tool that also seems to struggle with queer depiction.</p></div></div></div></div></div></div></body></html>