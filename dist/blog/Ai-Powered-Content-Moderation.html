<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><link rel="shortcut icon" type="image/png" href="/innovateus_favicon.001.png"><link rel="stylesheet" href="https://kit.fontawesome.com/59ddbfe387.css" crossorigin="anonymous"><link href="https://cdn.jsdelivr.net/npm/@mdi/font@5.x/css/materialdesignicons.min.css" rel="stylesheet"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0"><meta name="viewport" content="width=device-width,initial-scale=1"><script type="module" async="" crossorigin="" src="./assets/app-Do0VHJz9.js"></script><link rel="stylesheet" crossorigin="" href="./assets/app-CYsdt1SR.css"><link rel="modulepreload" crossorigin="" href="./assets/_slug_-qPMvHsKI.js"><title>RebootDemocracy.AI Blog | To Fight Extremism and Hate Speech Online, Invest in AI-Powered Content Moderation</title><meta name="title" content="RebootDemocracy.AI Blog | To Fight Extremism and Hate Speech Online, Invest in AI-Powered Content Moderation"><meta name="description" content="In an year marked by escalating online hate, a presidential election, and the dismantling of online safety teams, the adoption of AI-powered solutions could be pivotal. Can advanced AI provide the nuanced, scalable defense our digital communities desperately need against the tide of extremism and disinformation?"><meta property="og:title" content="RebootDemocracy.AI Blog | To Fight Extremism and Hate Speech Online, Invest in AI-Powered Content Moderation"><meta property="og:description" content="In an year marked by escalating online hate, a presidential election, and the dismantling of online safety teams, the adoption of AI-powered solutions could be pivotal. Can advanced AI provide the nuanced, scalable defense our digital communities desperately need against the tide of extremism and disinformation?"><meta property="og:image" content="https://content.thegovlab.com/assets/9197e265-1b1b-42d8-bc2c-7886fdb75207.webp"><meta property="og:image:width" content="800"><meta property="og:image:height" content="800"><meta property="twitter:title" content="RebootDemocracy.AI Blog | To Fight Extremism and Hate Speech Online, Invest in AI-Powered Content Moderation"><meta property="twitter:description" content="In an year marked by escalating online hate, a presidential election, and the dismantling of online safety teams, the adoption of AI-powered solutions could be pivotal. Can advanced AI provide the nuanced, scalable defense our digital communities desperately need against the tide of extremism and disinformation?"><meta property="twitter:image" content="https://content.thegovlab.com/assets/9197e265-1b1b-42d8-bc2c-7886fdb75207.webp"><meta property="twitter:card" content="summary_large_image"></head><body><div id="app" class="grid justify-items-stretch" data-server-rendered="true"><div class="app-container" data-v-390d8bb0=""><div class="main-content" data-v-390d8bb0=""><div data-v-390d8bb0=""><div><h1>To Fight Extremism and Hate Speech Online, Invest in AI-Powered Content Moderation</h1><div><p dir="ltr">Last month, Forbes <a href="https://www.forbes.com/sites/thomasbrewster/2024/01/10/elon-musk-fired-80-per-cent-of-twitter-x-engineers-working-on-trust-and-safety/#:~:text=Elon%20Musk%20fired%2080%25%20of,eSafety%2C%20Australia's%20online%20safety%20commissioner.">covered</a> a report from Australia’s online safety commissioner, which revealed that Elon Musk – since his reign at X (formerly known as Twitter) began – has fired over 80% of the platform’s “trust and safety” (T&amp;S) engineers and a third of the non-engineers working on the same T&amp;S team. Musk’s firing of over 1,000 of these employees since 2022 and his crusade to reinstate banned accounts has created a “perfect storm” for the spread of abusive content online, in the watchdog’s own words.</p><p dir="ltr">This is not an update to ignore. Online hate has only <a href="https://www.bbc.com/news/newsbeat-59292509">grown</a> since the pandemic, and may we be reminded that hate crimes only <a href="https://thegrio.com/2023/04/19/report-finds-hate-crimes-increased-during-presidential-elections/">surge</a> during election years. After 2020’s Trump-Biden showdown, anti-Black hate crimes increased by 14%, anti-Hispanic or anti-Latino hate crimes increased by 35%, and anti-Asian hate increased by 168%.</p><p dir="ltr">The approaching 2024 election promises <a href="https://apnews.com/article/depape-paul-pelosi-qanon-conspiracy-theories-violence-390ad310fa34b0edb925d88540a7ddcd">new waves</a> of political extremism, while the pandemic has forced us to <a href="https://virtual-communities.thegovlab.org/">find community</a> in our online spaces more than ever before. Yet, the digital commons remain unsafe. With the backdrop of <a href="https://www.cnbc.com/2023/01/18/tech-layoffs-microsoft-amazon-meta-others-have-cut-more-than-60000.html">significant layoffs</a> at major social media platforms, a reduction in the workforce dedicated to content moderation, and a <a href="https://www.theguardian.com/media/2023/dec/07/2024-elections-social-media-content-safety-policies-moderation">rollback of policies</a> aimed at safeguarding online spaces, the question arises: how can we hope to combat this digital tide of hate and extremism?</p><p dir="ltr">The answer may just lie in AI-powered content moderation.</p><p dir="ltr">&nbsp;</p><h3 dir="ltr">Old Dog, New Updates</h3><p dir="ltr">Traditional content moderation relies heavily on human moderators to analyze and manage the vast majority of content that goes against a platform’s community guidelines – whether hate speech, disinformation, harassment, or other inappropriate behaviors. Not only does manual oversight require a vast “suck” of resources from the platform, but trawling through offensive content is&nbsp; incredibly draining and difficult on human moderators.&nbsp;</p><p dir="ltr">To supplement workers, many social media platforms have been using some form of algorithm or machine-learning moderator for decades. Helping sift through the tidal wave of content, some tools use keywords, black lists, and flags to remove content as it's posted and reduce the load on human moderators. Certain algorithms count flesh-toned pixels to filter nudity or pornography. However, research has <a href="https://cse.engin.umich.edu/stories/researchers-leverage-ai-to-fight-online-hate-speech">shown </a>that simpler, “rule-based” algorithms are “inherently fragile to the nuances of natural language.”</p><blockquote><p dir="ltr">It begs asking – what would happen if these platforms instead more heavily deputized data-driven, deep learning methods?&nbsp;</p></blockquote><p dir="ltr">Such deep neural networks, more similar to the AI tools we’ve seen flourish this year, are able to learn richer, more precise representations and extrapolate these representations to new data. Data can teach them to recognize patterns, nuances, and anomalies indicative of hate speech, disinformation, and extremist propaganda – say, from open-source datasets like <a href="https://github.com/paul-rottger/hatecheck-data">HateCheck</a>, compiled for research using AI in hate speech detection.</p><p dir="ltr">AI-powered content moderation offers several compelling benefits over the resource-intensive human system. Scalability easily tops that list, allowing moderation-focused programs to keep pace with the explosive growth of online content as digital first-responders struggle to stay ahead. The urgency is made even more pressing by the <a href="https://www.businessinsider.com/ai-scam-spam-hacking-ruining-internet-chatgpt-privacy-misinformation-2023-8">new need</a> to combat tidal waves of AI-generated spam and disinformation. Why not "fight fire with fire?"&nbsp;</p><p dir="ltr"><img src="https://content.thegovlab.com/assets/657aeed8-031e-4364-a7b7-852996691ff8?width=1840&amp;height=1304" alt="An Intelligence Fueled Approachto Content Moderation 1"></p><p dir="ltr">AI systems can also review content as it's posted. The speed of AI tools is particularly crucial for moderating live streams, a place where social media platforms have consistently struggled to detect and mitigate harmful content. An AI-powered approach can automatically detect any harmful cases before they go live. Look at online shopping in China, where the Chinese government is <a href="https://www.bloomberg.com/opinion/articles/2023-11-22/china-crackdown-live-streaming-is-now-on-life-support">struggling to keep up</a> with the multitude of gamers and influencers now selling products <a href="https://www.cnbc.com/2023/12/11/chinas-livestream-shopping-booms-fueling-new-tech-like-avatars-and-ai.html">through digitally-created avatars</a> in 24/7 live streams.</p><p dir="ltr">To be sure, LLMs are expensive to run and operate, so it’s unlikely any platform could be meaningfully moderated exclusively via generative AI. Instead, social media companies should construct and rely on a series of smaller, targeted tools that use machine-learning to precisely moderate one aspect of disallowed content. For example, Facebook’s content moderation rules have recently shifted from its former&nbsp; <a href="https://www.propublica.org/article/facebook-hate-speech-censorship-internal-documents-algorithms">“colorblind”</a> approach to one that <a href="https://www.forbes.com/sites/jemimamcevoy/2020/12/03/on-facebook-comments-about-whites-men-and-americans-will-face-less-moderation/?sh=454068ad21a7">weighs identity categories differently</a>; they could further this effort by separately training a series of targeted AI tools on the complex dynamics of how different minoritized populations experience oppression and hate online, with each tool tasked to protect a different identity group.</p><p><strong>&nbsp;</strong></p><h3 dir="ltr">Learning Systems</h3><p dir="ltr">Bias, while mitigated, cannot be entirely eliminated. That’s why it’s hugely consequential that these AI-powered systems can learn. Through continuous feedback loops with human moderators, these systems can <a href="https://www.fastcompany.com/90926276/we-may-not-be-able-to-eliminating-bias-in-ai-but-we-can-at-least-tame-the-tech">adjust</a>, reducing bias and increasing effectiveness over time. AI systems can then turn around and immediately implement that feedback, ensuring a quicker and fairer route to a more impartial moderation process. While it’s not a cure for bias, it’s absolutely a salve that can only be improved over time.</p><p dir="ltr">Additionally, we must be wary of <a href="https://aicontentfy.com/en/blog/role-of-ai-in-content-moderation-and-censorship#:~:text=The%20use%20of%20AI%20in%20content%20moderation%20and%20censorship%20raises,or%20censorship%20of%20legitimate%20speech.">over-censorship</a>, especially in a pre-moderation system. Having AI systems rely on years of existing data and guidelines might lead to excessive homogenization, potentially stifling free expression. Again, that’s the necessity of our “AI on tap, not on top” approach, which centralizes decision-making and course alteration in humanity while outsourcing task-based functions to AI-powered assistants. Self-effacing reliance on previous data can be mitigated by human feedback on old patterns or new threats – which AI systems are particularly suited to quickly meet the challenge of.</p><p dir="ltr">The automation of natural language processing represents a vital tool in the digital arsenal. AI-powered content moderation systems that are built and trained thoughtfully will be a powerful tool in scaling to meet the ever-growing amount of online content, filtering out hate speech and disinformation ahead of the 2024 elections, protecting human moderators, and sharpening our ability to adapt to new harms.</p><p>&nbsp;</p></div></div></div></div></div></div></body></html>